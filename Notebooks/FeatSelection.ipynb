{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "087a81d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bool_col_0</th>\n",
       "      <th>bool_col_1</th>\n",
       "      <th>bool_col_2</th>\n",
       "      <th>bool_col_3</th>\n",
       "      <th>bool_col_4</th>\n",
       "      <th>int_col_0</th>\n",
       "      <th>int_col_1</th>\n",
       "      <th>int_col_2</th>\n",
       "      <th>int_col_3</th>\n",
       "      <th>int_col_4</th>\n",
       "      <th>...</th>\n",
       "      <th>cat_col_1</th>\n",
       "      <th>cat_col_2</th>\n",
       "      <th>cat_col_3</th>\n",
       "      <th>cat_col_4</th>\n",
       "      <th>date_col_0</th>\n",
       "      <th>date_col_1</th>\n",
       "      <th>date_col_2</th>\n",
       "      <th>date_col_3</th>\n",
       "      <th>date_col_4</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>946</td>\n",
       "      <td>839</td>\n",
       "      <td>945</td>\n",
       "      <td>801</td>\n",
       "      <td>955</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>D</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>2025-08-29</td>\n",
       "      <td>2020-05-08</td>\n",
       "      <td>2028-12-02</td>\n",
       "      <td>2025-11-30</td>\n",
       "      <td>2029-12-21</td>\n",
       "      <td>12.137458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>686</td>\n",
       "      <td>123</td>\n",
       "      <td>70</td>\n",
       "      <td>588</td>\n",
       "      <td>599</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>2027-09-07</td>\n",
       "      <td>2024-01-10</td>\n",
       "      <td>2025-05-17</td>\n",
       "      <td>2026-10-08</td>\n",
       "      <td>2020-09-21</td>\n",
       "      <td>488.643897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>750</td>\n",
       "      <td>992</td>\n",
       "      <td>363</td>\n",
       "      <td>809</td>\n",
       "      <td>755</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>2025-02-06</td>\n",
       "      <td>2027-04-15</td>\n",
       "      <td>2026-12-04</td>\n",
       "      <td>2028-02-13</td>\n",
       "      <td>2020-11-28</td>\n",
       "      <td>644.205482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>771</td>\n",
       "      <td>112</td>\n",
       "      <td>569</td>\n",
       "      <td>648</td>\n",
       "      <td>380</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>C</td>\n",
       "      <td>2025-06-02</td>\n",
       "      <td>2025-09-13</td>\n",
       "      <td>2022-07-10</td>\n",
       "      <td>2026-03-27</td>\n",
       "      <td>2026-02-13</td>\n",
       "      <td>918.191773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>902</td>\n",
       "      <td>11</td>\n",
       "      <td>801</td>\n",
       "      <td>617</td>\n",
       "      <td>328</td>\n",
       "      <td>...</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>D</td>\n",
       "      <td>2024-10-02</td>\n",
       "      <td>2027-11-06</td>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>2029-03-08</td>\n",
       "      <td>2022-12-22</td>\n",
       "      <td>381.335817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bool_col_0  bool_col_1  bool_col_2  bool_col_3  bool_col_4  int_col_0  \\\n",
       "0        True       False        True       False       False        946   \n",
       "1       False        True       False       False        True        686   \n",
       "2        True        True       False       False        True        750   \n",
       "3        True        True       False       False        True        771   \n",
       "4        True        True       False        True       False        902   \n",
       "\n",
       "   int_col_1  int_col_2  int_col_3  int_col_4  ...  cat_col_1  cat_col_2  \\\n",
       "0        839        945        801        955  ...          C          D   \n",
       "1        123         70        588        599  ...          A          D   \n",
       "2        992        363        809        755  ...          C          A   \n",
       "3        112        569        648        380  ...          C          B   \n",
       "4         11        801        617        328  ...          B          A   \n",
       "\n",
       "   cat_col_3  cat_col_4  date_col_0 date_col_1 date_col_2 date_col_3  \\\n",
       "0          B          B  2025-08-29 2020-05-08 2028-12-02 2025-11-30   \n",
       "1          A          A  2027-09-07 2024-01-10 2025-05-17 2026-10-08   \n",
       "2          C          C  2025-02-06 2027-04-15 2026-12-04 2028-02-13   \n",
       "3          D          C  2025-06-02 2025-09-13 2022-07-10 2026-03-27   \n",
       "4          C          D  2024-10-02 2027-11-06 2023-12-01 2029-03-08   \n",
       "\n",
       "  date_col_4      target  \n",
       "0 2029-12-21   12.137458  \n",
       "1 2020-09-21  488.643897  \n",
       "2 2020-11-28  644.205482  \n",
       "3 2026-02-13  918.191773  \n",
       "4 2022-12-22  381.335817  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import string\n",
    "\n",
    "def generate_synthetic_data(rows=1000, cols=30, seed=42):\n",
    "    \"\"\"\n",
    "    Generates a synthetic DataFrame with mixed data types:\n",
    "    - Boolean\n",
    "    - Integer (numeric)\n",
    "    - Float\n",
    "    - Text\n",
    "    - Categorical\n",
    "    - Date\n",
    "\n",
    "    Parameters:\n",
    "    - rows: int, number of rows\n",
    "    - cols: int, total number of columns\n",
    "\n",
    "    Returns:\n",
    "    - df: pd.DataFrame, synthetic dataset\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    col_types = ['bool', 'int', 'float', 'text', 'category', 'date']\n",
    "    type_counts = {k: cols // len(col_types) for k in col_types}\n",
    "    remainder = cols % len(col_types)\n",
    "    for i in range(remainder):\n",
    "        type_counts[col_types[i]] += 1\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    for i in range(type_counts['bool']):\n",
    "        data[f'bool_col_{i}'] = np.random.choice([True, False], size=rows)\n",
    "\n",
    "    for i in range(type_counts['int']):\n",
    "        data[f'int_col_{i}'] = np.random.randint(0, 1000, size=rows)\n",
    "\n",
    "    for i in range(type_counts['float']):\n",
    "        data[f'float_col_{i}'] = np.random.uniform(0, 1000, size=rows)\n",
    "\n",
    "    for i in range(type_counts['text']):\n",
    "        data[f'text_col_{i}'] = [''.join(random.choices(string.ascii_letters, k=10)) for _ in range(rows)]\n",
    "\n",
    "    for i in range(type_counts['category']):\n",
    "        data[f'cat_col_{i}'] = pd.Series(np.random.choice(['A', 'B', 'C', 'D'], size=rows)).astype(\"category\")\n",
    "\n",
    "    base_date = datetime(2020, 1, 1)\n",
    "    for i in range(type_counts['date']):\n",
    "        data[f'date_col_{i}'] = [base_date + timedelta(days=np.random.randint(0, 3650)) for _ in range(rows)]\n",
    "\n",
    "    data[f'target'] = np.random.uniform(0, 1000, size=rows)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df['target'] = df['target'].astype(float)\n",
    "    return df\n",
    "\n",
    "# Generate and preview\n",
    "main_df = generate_synthetic_data()\n",
    "df = main_df.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc1c8e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\varun\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "c:\\Users\\varun\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\varun\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def convert_str_to_numeric(df, errors='coerce'):\n",
    "    \"\"\"\n",
    "    Converts columns with strings that look like numbers into numeric dtype.\n",
    "    `errors='coerce'` will convert invalid parsing to NaN.\n",
    "    \"\"\"\n",
    "    df_converted = df.copy()\n",
    "    for col in df_converted.columns:\n",
    "        if df_converted[col].dtype == object:\n",
    "            try:\n",
    "                df_converted[col] = pd.to_numeric(df_converted[col], errors=errors)\n",
    "            except:\n",
    "                pass\n",
    "    return df_converted\n",
    "\n",
    "def lowercase_string_columns(df):\n",
    "    \"\"\"\n",
    "    Lowercases all string (object) column values.\n",
    "    \"\"\"\n",
    "    df_cleaned = df.copy()\n",
    "    str_cols = df_cleaned.select_dtypes(include='object').columns\n",
    "    for col in str_cols:\n",
    "        df_cleaned[col] = df_cleaned[col].astype(str).str.lower().str.strip()\n",
    "    return df_cleaned\n",
    "\n",
    "def handle_boolean_columns(df):\n",
    "    \"\"\"\n",
    "    Converts columns with boolean-looking strings or integers into proper bool dtype.\n",
    "    \"\"\"\n",
    "    df_bool = df.copy()\n",
    "    for col in df_bool.columns:\n",
    "        if df_bool[col].dtype == object:\n",
    "            unique_vals = df_bool[col].dropna().astype(str).str.lower().unique()\n",
    "            if set(unique_vals).issubset({'true', 'false', 'yes', 'no', '1', '0'}):\n",
    "                df_bool[col] = df_bool[col].astype(str).str.lower().map({\n",
    "                    'true': True, '1': True, 'yes': True,\n",
    "                    'false': False, '0': False, 'no': False\n",
    "                })\n",
    "        elif df_bool[col].dtype in [int, float] and set(df_bool[col].dropna().unique()).issubset({0, 1}):\n",
    "            df_bool[col] = df_bool[col].astype(bool)\n",
    "    return df_bool\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "def normalize_minmax(df, columns=None):\n",
    "    \"\"\"Applies Min-Max normalization to selected numeric columns.\"\"\"\n",
    "    df_norm = df.copy()\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=[np.number]).columns\n",
    "    scaler = MinMaxScaler()\n",
    "    df_norm[columns] = scaler.fit_transform(df_norm[columns])\n",
    "    return df_norm\n",
    "\n",
    "def standardize_zscore(df, columns=None):\n",
    "    \"\"\"Standardizes selected numeric columns using Z-score (mean=0, std=1).\"\"\"\n",
    "    df_scaled = df.copy()\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=[np.number]).columns\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled[columns] = scaler.fit_transform(df_scaled[columns])\n",
    "    return df_scaled\n",
    "\n",
    "def robust_scale(df, columns=None):\n",
    "    \"\"\"Applies robust scaling using median and IQR (resistant to outliers).\"\"\"\n",
    "    df_robust = df.copy()\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=[np.number]).columns\n",
    "    scaler = RobustScaler()\n",
    "    df_robust[columns] = scaler.fit_transform(df_robust[columns])\n",
    "    return df_robust\n",
    "\n",
    "def log_transform(df, columns=None, add_constant=True):\n",
    "    \"\"\"\n",
    "    Applies log transformation to selected numeric columns.\n",
    "    - add_constant: Add 1 to avoid log(0) if True.\n",
    "    \"\"\"\n",
    "    df_log = df.copy()\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in columns:\n",
    "        try:\n",
    "            if add_constant:\n",
    "                df_log[col] = np.log1p(df_log[col])  # log(1 + x)\n",
    "            else:\n",
    "                df_log[col] = np.log(df_log[col])\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping log transform on {col} due to error: {e}\")\n",
    "    return df_log\n",
    "def drop_duplicates(df):\n",
    "    \"\"\"Remove duplicate rows from the DataFrame.\"\"\"\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "def drop_constant_columns(df):\n",
    "    \"\"\"Remove columns with only a single unique value.\"\"\"\n",
    "    return df.loc[:, df.apply(lambda col: col.nunique(dropna=False) > 1)]\n",
    "\n",
    "def drop_columns_by_null_threshold(df, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Drop columns with missing values above the given threshold.\n",
    "    - threshold (float): Max allowable proportion of nulls (0 to 1).\n",
    "    \"\"\"\n",
    "    null_fraction = df.isnull().mean()\n",
    "    cols_to_keep = null_fraction[null_fraction <= threshold].index\n",
    "    return df[cols_to_keep]\n",
    "\n",
    "def impute_missing_values(df, strategy=\"mean\"):\n",
    "    \"\"\"\n",
    "    Fill missing values in numeric and categorical columns.\n",
    "    - strategy (str): 'mean', 'median', or 'mode'.\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            if strategy == \"mean\":\n",
    "                fill_value = df[col].mean()\n",
    "            elif strategy == \"median\":\n",
    "                fill_value = df[col].median()\n",
    "            elif strategy == \"mode\":\n",
    "                fill_value = df[col].mode()[0]\n",
    "            else:\n",
    "                raise ValueError(\"strategy must be one of: 'mean', 'median', 'mode'\")\n",
    "            df_clean[col] = df[col].fillna(fill_value)\n",
    "        else:\n",
    "            # Categorical or text-based column: use mode only\n",
    "            fill_value = df[col].mode()[0]\n",
    "            df_clean[col] = df[col].fillna(fill_value)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def strip_whitespace_string_columns(df):\n",
    "    \"\"\"Trim whitespace in string or object-type columns.\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    str_cols = df_clean.select_dtypes(include=[\"object\", \"string\"]).columns\n",
    "    for col in str_cols:\n",
    "        df_clean[col] = df_clean[col].astype(str).str.strip()\n",
    "    return df_clean\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "date_cols = df.select_dtypes(include=[\"datetime64\"]).columns.tolist()\n",
    "bool_cols = df.select_dtypes(include=[\"bool\"]).columns.tolist()\n",
    "all_cols = df.columns.tolist()\n",
    "\n",
    "df = drop_duplicates(df)\n",
    "df = drop_constant_columns(df)\n",
    "df = drop_columns_by_null_threshold(df, threshold=0.4)\n",
    "df = impute_missing_values(df, strategy=\"mode\")\n",
    "df = strip_whitespace_string_columns(df)\n",
    "df = normalize_minmax(df, columns=numeric_cols)\n",
    "df = standardize_zscore(df, columns=numeric_cols)\n",
    "df = robust_scale(df, columns=numeric_cols)\n",
    "df = log_transform(df, columns=numeric_cols, add_constant=True)\n",
    "df = log_transform(df, columns=numeric_cols, add_constant=False)\n",
    "df = convert_str_to_numeric(df, errors='coerce')\n",
    "df = convert_str_to_numeric(df, errors='ignore')\n",
    "df = convert_str_to_numeric(df, errors='raise')\n",
    "df = lowercase_string_columns(df)\n",
    "df = handle_boolean_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "133dfe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def split_regression_data(df, target_col, method=\"random\", test_size=0.2, stratify_bins=10, time_col=None, n_clusters=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Split regression dataset using different strategies: random, stratified, time-based, or cluster-based.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input dataset.\n",
    "    - target_col (str): Name of the regression target column.\n",
    "    - method (str): One of ['random', 'stratified', 'time', 'cluster'].\n",
    "    - test_size (float): Proportion of test set.\n",
    "    - stratify_bins (int): Number of quantile bins for stratified splitting.\n",
    "    - time_col (str): Column for time-based split (required for 'time' method).\n",
    "    - n_clusters (int): Number of clusters for cluster-based split.\n",
    "    - random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - train_df (pd.DataFrame): Training data.\n",
    "    - test_df (pd.DataFrame): Testing data.\n",
    "    \"\"\"\n",
    "    df = df.dropna(subset=[target_col]).copy()\n",
    "\n",
    "    if method == \"random\":\n",
    "        train_df, test_df = train_test_split(df, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    elif method == \"stratified\":\n",
    "        # Bin the target column for stratification\n",
    "        stratify_labels = pd.qcut(df[target_col], q=stratify_bins, duplicates=\"drop\")\n",
    "        train_df, test_df = train_test_split(df, test_size=test_size, stratify=stratify_labels, random_state=random_state)\n",
    "\n",
    "    elif method == \"time\":\n",
    "        if time_col is None:\n",
    "            raise ValueError(\"`time_col` must be provided for time-based splitting.\")\n",
    "        df = df.sort_values(time_col)\n",
    "        split_idx = int((1 - test_size) * len(df))\n",
    "        train_df, test_df = df.iloc[:split_idx], df.iloc[split_idx:]\n",
    "\n",
    "    elif method == \"cluster\":\n",
    "        feature_cols = df.drop(columns=[target_col]).select_dtypes(include=np.number).columns\n",
    "        if len(feature_cols) == 0:\n",
    "            raise ValueError(\"Cluster-based splitting requires numerical features.\")\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(df[feature_cols])\n",
    "\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n",
    "        df[\"cluster\"] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "        # Stratified split using cluster labels\n",
    "        train_df, test_df = train_test_split(df, test_size=test_size, stratify=df[\"cluster\"], random_state=random_state)\n",
    "        df.drop(columns=[\"cluster\"], inplace=True)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid split method: {method}. Choose from ['random', 'stratified', 'time', 'cluster'].\")\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "train, test = split_regression_data(df, target_col='target', method='random', test_size=0.3, random_state=42)\n",
    "X_train = train.drop(columns=['target'])\n",
    "y_train = train['target']\n",
    "X_test = test.drop(columns=['target'])\n",
    "y_test = test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af147f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_regression, SelectFromModel, RFE, VarianceThreshold\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "def select_k_best(X, y, k=10, score_func=f_regression):\n",
    "    selector = SelectKBest(score_func=score_func, k=k)\n",
    "    selector.fit(X, y)\n",
    "    selected = selector.get_support()\n",
    "    return X.loc[:, selected]\n",
    "\n",
    "def lasso_feature_selection(X, y, alpha=0.01):\n",
    "    lasso = LassoCV(alphas=[alpha]).fit(X, y)\n",
    "    model = SelectFromModel(lasso, prefit=True)\n",
    "    return X.loc[:, model.get_support()]\n",
    "\n",
    "def rfe_selection(X, y, estimator=RandomForestRegressor(), n_features=10, step=1):\n",
    "    selector = RFE(estimator, n_features_to_select=n_features, step=step)\n",
    "    selector.fit(X, y)\n",
    "    return X.loc[:, selector.get_support()]\n",
    "\n",
    "def random_forest_importance(X, y, n_estimators=100):\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators)\n",
    "    model.fit(X, y)\n",
    "    importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "    return importances.sort_values(ascending=False)\n",
    "\n",
    "def vif_selection(X, thresh=5.0):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data['feature'] = X.columns\n",
    "    vif_data['VIF'] = [variance_inflation_factor(X_scaled, i) for i in range(X.shape[1])]\n",
    "    return vif_data[vif_data['VIF'] < thresh]\n",
    "\n",
    "def knn_feature_importance(X, y, n_neighbors=5, n_features=10, random_state=42):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    feature_scores = {}\n",
    "    for feature in X.columns:\n",
    "        model = KNeighborsRegressor(n_neighbors=n_neighbors)\n",
    "        X_feature = X_scaled[:, X.columns.get_loc(feature)].reshape(-1, 1)\n",
    "        model.fit(X_feature, y)\n",
    "        score = model.score(X_feature, y)\n",
    "        feature_scores[feature] = score\n",
    "    selected_features = pd.Series(feature_scores).nlargest(n_features).index.tolist()\n",
    "    return selected_features\n",
    "\n",
    "def variance_threshold_selector(X, threshold=0.0):\n",
    "    selector = VarianceThreshold(threshold=threshold)\n",
    "    selector.fit(X)\n",
    "    return X.loc[:, selector.get_support()]\n",
    "\n",
    "def univariate_feature_selection(X, y, score_func=chi2, k=10):\n",
    "    selector = SelectKBest(score_func=score_func, k=k)\n",
    "    selector.fit(X, y)\n",
    "    return X.loc[:, selector.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b4e577",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Step 1: Drop text/date columns (optional: encode dates separately)\n",
    "non_numeric_cols = X_train.select_dtypes(include=['object', 'category', 'datetime']).columns\n",
    "X_train_numeric = X_train.drop(columns=non_numeric_cols)\n",
    "\n",
    "# Step 2: Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train_numeric), columns=X_train_numeric.columns)\n",
    "\n",
    "# Step 3: Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_imputed), columns=X_train_imputed.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79feac2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Selected Features: Index(['bool_col_0', 'bool_col_1', 'bool_col_2', 'bool_col_3', 'int_col_0',\n",
      "       'int_col_1', 'int_col_3', 'int_col_4', 'float_col_0', 'float_col_1',\n",
      "       'float_col_2', 'float_col_3', 'float_col_4'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 2. Lasso Feature Selection\n",
    "X_lasso = lasso_feature_selection(X_train_scaled, y_train, alpha=0.01)\n",
    "print(\"Lasso Selected Features:\", X_lasso.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c51a3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFE Selected Features: Index(['int_col_0', 'int_col_1', 'int_col_2', 'int_col_3', 'int_col_4',\n",
      "       'float_col_0', 'float_col_1', 'float_col_2', 'float_col_3',\n",
      "       'float_col_4'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 3. RFE (Recursive Feature Elimination)\n",
    "X_rfe = rfe_selection(X_train_scaled, y_train, estimator=RandomForestRegressor(), n_features=10, step=1)\n",
    "print(\"RFE Selected Features:\", X_rfe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6d5717c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Feature Importances: float_col_0    0.182666\n",
      "float_col_4    0.120645\n",
      "float_col_2    0.112471\n",
      "int_col_0      0.096471\n",
      "int_col_1      0.085069\n",
      "int_col_4      0.075175\n",
      "float_col_3    0.074871\n",
      "float_col_1    0.066909\n",
      "int_col_3      0.057254\n",
      "int_col_2      0.052044\n",
      "bool_col_0     0.017514\n",
      "bool_col_2     0.016165\n",
      "bool_col_4     0.015824\n",
      "bool_col_3     0.015686\n",
      "bool_col_1     0.011237\n",
      "text_col_0     0.000000\n",
      "text_col_1     0.000000\n",
      "text_col_2     0.000000\n",
      "text_col_3     0.000000\n",
      "text_col_4     0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 4. Random Forest Feature Importances\n",
    "rf_importance = random_forest_importance(X_train_scaled, y_train, n_estimators=100)\n",
    "print(\"Random Forest Feature Importances:\", rf_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "474f3670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIF Selected Features: ['bool_col_0' 'bool_col_1' 'bool_col_2' 'bool_col_3' 'bool_col_4'\n",
      " 'int_col_0' 'int_col_1' 'int_col_2' 'int_col_3' 'int_col_4' 'float_col_0'\n",
      " 'float_col_1' 'float_col_2' 'float_col_3' 'float_col_4']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\varun\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1754: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return 1 - self.ssr/self.uncentered_tss\n"
     ]
    }
   ],
   "source": [
    "# 5. Variance Inflation Factor (VIF)\n",
    "X_vif = vif_selection(X_train_scaled, thresh=5.0)\n",
    "print(\"VIF Selected Features:\", X_vif['feature'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5c1473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance Threshold Selected Features: Index(['bool_col_0', 'bool_col_1', 'bool_col_2', 'bool_col_3', 'bool_col_4',\n",
      "       'int_col_0', 'int_col_1', 'int_col_2', 'int_col_3', 'int_col_4',\n",
      "       'float_col_0', 'float_col_1', 'float_col_2', 'float_col_3',\n",
      "       'float_col_4'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 6. Variance Threshold Selector\n",
    "X_variance_threshold = variance_threshold_selector(X_train_scaled, threshold=0.0)\n",
    "print(\"Variance Threshold Selected Features:\", X_variance_threshold.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729fae0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Univariate (f_regression) Selected Features: Index(['bool_col_2', 'bool_col_3', 'int_col_0', 'int_col_1', 'int_col_3',\n",
      "       'int_col_4', 'float_col_0', 'float_col_2', 'float_col_3',\n",
      "       'float_col_4'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\varun\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:289: RuntimeWarning: invalid value encountered in divide\n",
      "  correlation_coefficient /= X_norms\n"
     ]
    }
   ],
   "source": [
    "# 7. Univariate Feature Selection (using Chi-Square)\n",
    "X_univariate = univariate_feature_selection(X_train_scaled, y_train, score_func=f_regression, k=10)\n",
    "print(\"Univariate (f_regression) Selected Features:\", X_univariate.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2813ccfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Selected Features: ['float_col_2', 'float_col_0', 'int_col_3', 'int_col_0', 'int_col_4', 'float_col_1', 'float_col_4', 'int_col_2', 'bool_col_2', 'bool_col_4']\n"
     ]
    }
   ],
   "source": [
    "# 8. KNN Feature Importance\n",
    "X_knn = knn_feature_importance(X_train_scaled, y_train, n_neighbors=5, n_features=10, random_state=42)\n",
    "print(\"KNN Selected Features:\", X_knn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
