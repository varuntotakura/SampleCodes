{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4100712",
   "metadata": {},
   "source": [
    "XGBoost for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37707fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "724c20d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 5263.2106\n",
      "Root Mean Squared Error: 72.5480\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error\n",
    "\n",
    "# Create synthetic regression data\n",
    "X, y = make_regression(n_samples=1000, n_features=208, noise=0.2, random_state=42)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train regressor\n",
    "regressor = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "print(f\"Root Mean Squared Error: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7829a750",
   "metadata": {},
   "source": [
    "XGBoost with Hyperparametric Tuning: \n",
    "RandomizedSearchCV to narrow the region, then GridSearchCV in that narrowed region for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27b9da31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Best params from RandomizedSearchCV: {'colsample_bytree': 0.5370223258670452, 'learning_rate': 0.11753971856328177, 'max_depth': 3, 'n_estimators': 77, 'subsample': 0.9315517129377968}\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "Best params from GridSearchCV: {'learning_rate': 0.14104766227593812, 'max_depth': 2, 'n_estimators': 97}\n",
      "Final Test MSE: 2098.7017\n",
      "Final Test RMSE: 45.8116\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "import joblib\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# RandomizedSearchCV: Broad search\n",
    "xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 300),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'subsample': uniform(0.5, 0.5),\n",
    "    'colsample_bytree': uniform(0.5, 0.5)\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb_reg, param_distributions=param_dist,\n",
    "    n_iter=20, scoring='neg_mean_squared_error', cv=3, verbose=1, random_state=42\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params from RandomizedSearchCV:\", random_search.best_params_)\n",
    "\n",
    "# GridSearchCV: Fine-tune in narrowed region\n",
    "narrowed_params = {\n",
    "    'n_estimators': [random_search.best_params_['n_estimators'] - 20, random_search.best_params_['n_estimators'], random_search.best_params_['n_estimators'] + 20],\n",
    "    'learning_rate': [random_search.best_params_['learning_rate'] * f for f in [0.8, 1.0, 1.2]],\n",
    "    'max_depth': [max(1, random_search.best_params_['max_depth'] - 1), random_search.best_params_['max_depth'], random_search.best_params_['max_depth'] + 1],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    xgb_reg, param_grid=narrowed_params,\n",
    "    scoring='neg_mean_squared_error', cv=3, verbose=1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params from GridSearchCV:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred = grid_search.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Final Test MSE: {mse:.4f}\")\n",
    "print(f\"Final Test RMSE: {np.sqrt(mse):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da25266",
   "metadata": {},
   "source": [
    "Incase of High Dimensional data: Using PCA to reduce dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ec0a4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
      "Test MSE: 28025.3628\n",
      "Test RMSE: 167.4078\n",
      "Best Parameters: {'pca__n_components': 15, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 3, 'xgb__n_estimators': 286}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Pipeline: StandardScaler -> PCA -> XGBoost\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=50)),  # Reduce dimensions to prevent overfitting\n",
    "    ('xgb', xgb.XGBRegressor(objective='reg:squarederror', random_state=42))\n",
    "])\n",
    "\n",
    "# Grid Search on selected hyperparameters\n",
    "param_grid = {\n",
    "    'pca__n_components': [5, 10, 15],  # Ensure n_components <= min(n_samples, n_features)\n",
    "    'xgb__n_estimators': [50, 286],\n",
    "    'xgb__max_depth': [3, 5],\n",
    "    'xgb__learning_rate': [0.01, 0.05, 0.1, 0.15],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best Model Evaluation\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Test MSE: {mse:.4f}\")\n",
    "print(f\"Test RMSE: {np.sqrt(mse):.4f}\")\n",
    "print(\"Best Parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e0c289",
   "metadata": {},
   "source": [
    "Using Support Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a25d78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "SVR Test MSE: 26973.6954\n",
      "Best Parameters: {'pca__n_components': 30, 'svr__C': 10, 'svr__epsilon': 0.01, 'svr__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Build the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=50)),\n",
    "    ('svr', SVR())\n",
    "])\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "param_grid = {\n",
    "    'pca__n_components': [30, 50, 80],\n",
    "    'svr__C': [0.1, 1, 10],\n",
    "    'svr__epsilon': [0.01, 0.1, 0.5],\n",
    "    'svr__kernel': ['rbf']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"SVR Test MSE: {mse:.4f}\")\n",
    "print(\"Best Parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eca0c35",
   "metadata": {},
   "source": [
    "Using ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27c4364b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "ElasticNet Test MSE: 18678.2579\n",
      "Best Parameters: {'elastic__alpha': 0.01, 'elastic__l1_ratio': 0.2, 'pca__n_components': 80}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Build the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=50)),\n",
    "    ('elastic', ElasticNet(max_iter=10000))\n",
    "])\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "param_grid = {\n",
    "    'pca__n_components': [30, 50, 80],\n",
    "    'elastic__alpha': [0.01, 0.1, 1.0],\n",
    "    'elastic__l1_ratio': [0.2, 0.5, 0.8],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"ElasticNet Test MSE: {mse:.4f}\")\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3a3f9e",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9104f43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to xgb_regression_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(grid_search.best_estimator_, \"xgb_regression_model.pkl\")\n",
    "print(\"Model saved to xgb_regression_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aab0cb",
   "metadata": {},
   "source": [
    "KFold for Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e404de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Test MSE: 2140.5240\n",
      "Test RMSE: 46.2658\n",
      "Best Parameters: {'xgb__learning_rate': 0.1, 'xgb__max_depth': 3, 'xgb__n_estimators': 150}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Base model for feature selection\n",
    "base_selector_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define pipeline: Standardization -> Feature Selection -> XGBoost\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('feature_select', SelectFromModel(base_selector_model, threshold=\"median\")),\n",
    "    ('xgb', xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# K-Fold Cross-validation\n",
    "kfold_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning grid\n",
    "param_grid = {\n",
    "    'xgb__n_estimators': [100, 150],\n",
    "    'xgb__max_depth': [3, 5],\n",
    "    'xgb__learning_rate': [0.05, 0.1]\n",
    "}\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=kfold_cv, scoring='neg_mean_squared_error', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Test MSE: {mse:.4f}\")\n",
    "print(f\"Test RMSE: {np.sqrt(mse):.4f}\")\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c650ce",
   "metadata": {},
   "source": [
    "SMOTE for Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84c7f3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Test MSE: 2626.3936\n",
      "Test RMSE: 51.2484\n",
      "Best Parameters: {'xgb__learning_rate': 0.1, 'xgb__max_depth': 3, 'xgb__n_estimators': 150}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "\n",
    "# Bin the target variable to simulate stratification and enable SMOTE\n",
    "bins = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile')\n",
    "y_binned = bins.fit_transform(y.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test, y_binned_train, _ = train_test_split(\n",
    "    X, y, y_binned, test_size=0.2, stratify=y_binned, random_state=42\n",
    ")\n",
    "\n",
    "# Apply SMOTE on binned targets\n",
    "sm = SMOTE(random_state=42)\n",
    "X_resampled, y_binned_resampled = sm.fit_resample(X_train, y_binned_train)\n",
    "\n",
    "# Recover original regression targets after SMOTE using index mapping\n",
    "# Map from binned label back to original y\n",
    "# Note: Here we average y_train within each bin to approximate back\n",
    "bin_means = pd.Series(y_train).groupby(y_binned_train).mean()\n",
    "y_resampled = [bin_means[int(lbl)] for lbl in y_binned_resampled]\n",
    "y_resampled = np.array(y_resampled)\n",
    "\n",
    "# Pipeline with Feature Selection and XGBoost Regressor\n",
    "base_selector_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('feature_select', SelectFromModel(base_selector_model, threshold=\"median\")),\n",
    "    ('xgb', xgb.XGBRegressor(objective='reg:squarederror', random_state=42))\n",
    "])\n",
    "\n",
    "# KFold Cross-Validation (not stratified because regression, but 5-fold)\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Grid Search\n",
    "param_grid = {\n",
    "    'xgb__n_estimators': [100, 150],\n",
    "    'xgb__max_depth': [3, 5],\n",
    "    'xgb__learning_rate': [0.05, 0.1],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='neg_mean_squared_error', verbose=1)\n",
    "grid_search.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Evaluate\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Test MSE: {mse:.4f}\")\n",
    "print(f\"Test RMSE: {np.sqrt(mse):.4f}\")\n",
    "print(\"Best Parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52064ce",
   "metadata": {},
   "source": [
    "Use the model in future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7a57993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE from loaded model: 26048.7442\n",
      "RMSE from loaded model: 161.3962\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load model\n",
    "model = joblib.load(\"xgb_regression_model.pkl\")\n",
    "\n",
    "# Load data\n",
    "# X, y\n",
    "\n",
    "# Predict\n",
    "preds = model.predict(X)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y, preds)\n",
    "print(f\"MSE from loaded model: {mse:.4f}\")\n",
    "print(f\"RMSE from loaded model: {np.sqrt(mse):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
