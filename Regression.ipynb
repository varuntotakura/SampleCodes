{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4100712",
   "metadata": {},
   "source": [
    "XGBoost for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37707fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724c20d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 513.5613\n",
      "Root Mean Squared Error: 22.6619\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error\n",
    "\n",
    "# Create synthetic regression data\n",
    "X, y = make_regression(n_samples=12000, n_features=208, noise=0.2, random_state=42)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train regressor\n",
    "regressor = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "print(f\"Root Mean Squared Error: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7829a750",
   "metadata": {},
   "source": [
    "XGBoost with Hyperparametric Tuning: \n",
    "RandomizedSearchCV to narrow the region, then GridSearchCV in that narrowed region for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b9da31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "import joblib\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# RandomizedSearchCV: Broad search\n",
    "xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 300),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'subsample': uniform(0.5, 0.5),\n",
    "    'colsample_bytree': uniform(0.5, 0.5)\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb_reg, param_distributions=param_dist,\n",
    "    n_iter=20, scoring='neg_mean_squared_error', cv=3, verbose=1, random_state=42\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params from RandomizedSearchCV:\", random_search.best_params_)\n",
    "\n",
    "# GridSearchCV: Fine-tune in narrowed region\n",
    "narrowed_params = {\n",
    "    'n_estimators': [random_search.best_params_['n_estimators'] - 20, random_search.best_params_['n_estimators'], random_search.best_params_['n_estimators'] + 20],\n",
    "    'learning_rate': [random_search.best_params_['learning_rate'] * f for f in [0.8, 1.0, 1.2]],\n",
    "    'max_depth': [max(1, random_search.best_params_['max_depth'] - 1), random_search.best_params_['max_depth'], random_search.best_params_['max_depth'] + 1],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    xgb_reg, param_grid=narrowed_params,\n",
    "    scoring='neg_mean_squared_error', cv=3, verbose=1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params from GridSearchCV:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred = grid_search.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Final Test MSE: {mse:.4f}\")\n",
    "print(f\"Final Test RMSE: {np.sqrt(mse):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da25266",
   "metadata": {},
   "source": [
    "Incase of High Dimensional data: Using PCA to reduce dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec0a4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
      "Test MSE: 20334.6999\n",
      "Test RMSE: 142.5998\n",
      "Best Parameters: {'pca__n_components': 15, 'xgb__learning_rate': 0.05, 'xgb__max_depth': 3, 'xgb__n_estimators': 286}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Pipeline: StandardScaler -> PCA -> XGBoost\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=50)),  # Reduce dimensions to prevent overfitting\n",
    "    ('xgb', xgb.XGBRegressor(objective='reg:squarederror', random_state=42))\n",
    "])\n",
    "\n",
    "# Grid Search on selected hyperparameters\n",
    "param_grid = {\n",
    "    'pca__n_components': [5, 10, 15],  # Ensure n_components <= min(n_samples, n_features)\n",
    "    'xgb__n_estimators': [50, 286],\n",
    "    'xgb__max_depth': [3, 5],\n",
    "    'xgb__learning_rate': [0.01, 0.05, 0.1, 0.15],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best Model Evaluation\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Test MSE: {mse:.4f}\")\n",
    "print(f\"Test RMSE: {np.sqrt(mse):.4f}\")\n",
    "print(\"Best Parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3a3f9e",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9104f43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "joblib.dump(grid_search.best_estimator_, \"xgb_boston_model.pkl\")\n",
    "print(\"Model saved to xgb_boston_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52064ce",
   "metadata": {},
   "source": [
    "Use the model in future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a57993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load model\n",
    "model = joblib.load(\"xgb_boston_model.pkl\")\n",
    "\n",
    "# Load data\n",
    "# X, y\n",
    "\n",
    "# Predict\n",
    "preds = model.predict(X)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y, preds)\n",
    "print(f\"MSE from loaded model: {mse:.4f}\")\n",
    "print(f\"RMSE from loaded model: {np.sqrt(mse):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
