# Data Configuration
data:
  input_file: "housing.csv"  # Now points to our downloaded dataset
  target_column: "target"
  test_size: 0.2
  random_state: 42
  synthetic_data:
    enable: false

# Mode Configuration
mode:
  use_inf_as_null: false
  handle_outliers: true

# Preprocessing Configuration
preprocessing:
  missing_values:
    strategy: "auto"
    custom_rules: {}
  scaling:
    method: "standard"
    target_range: [-1, 1]

# Feature Engineering
feature_engineering:
  enable: true
  methods:
    derived_features:
      enable: true
      operations:
        - name: "rooms_per_household"
          columns: ["AveRooms", "AveOccup"]
          operation: "ratio"
        - name: "bedrooms_ratio"
          columns: ["AveBedrms", "AveRooms"]
          operation: "ratio"
    interactions:
      enable: true
      feature_pairs: [["MedInc", "AveRooms"], ["Population", "AveOccup"]]
      interaction_type: "multiplication"
    pca:
      enable: true
      n_components: 5
    polynomial:
      enable: true
      degree: 2
      interaction_only: false

# Feature Selection
feature_selection:
  enable: true
  methods:
    - name: k_best
      enable: true
      params:
        k: 5  # Changed from 10 to 5 since we only have 8 features
        score_func: f_regression
    - name: lasso
      enable: true
      params:
        alpha: [0.001, 0.01, 0.1, 1.0]
        threshold: 0.01
    - name: random_forest
      enable: true
      params:
        n_estimators: 50
        threshold: 0.01
    - name: vif
      enable: true
      params:
        threshold: 5.0
    - name: rfe
      enable: true
      params:
        n_features_to_select: 5
        step: 1

# Model Configuration - Using smaller parameter grids for faster execution
models:
  - name: 'xgboost'
    enable: true
    fixed_params:
      objective: "reg:squarederror"
      nthread: -1
      tree_method: "hist"  # Added this parameter to fix categorical data issue
    params:
      max_depth: [3, 5]
      learning_rate: [0.1]
      n_estimators: [100]
      subsample: [0.8]
      colsample_bytree: [0.8]
      min_child_weight: [1]

  - name: 'adaboost'
    enable: true
    params:
      n_estimators: [50]
      learning_rate: [0.1]
      loss: ['linear']

  - name: 'linear'
    enable: true
    params:
      fit_intercept: [true]
      normalize: [true]
      copy_X: [true]
      n_jobs: [-1]
      positive: [false]

  - name: 'lasso'
    enable: True
    params:
      alpha: [0.1]
      fit_intercept: [True]
      max_iter: [1000]
      tol: [0.0001]
      selection: ['cyclic']
      random_state: [42]

  - name: 'ridge'
    enable: True
    params:
      alpha: [0.1]
      fit_intercept: [True]
      max_iter: [1000]
      tol: [0.001]
      solver: ['auto']
      random_state: [42]

  - name: 'elastic_net'
    enable: True
    params:
      alpha: [0.1]
      l1_ratio: [0.5]
      fit_intercept: [True]
      max_iter: [1000]
      tol: [0.0001]
      selection: ['cyclic']
      random_state: [42]

  - name: 'svr'
    enable: True
    params:
      C: [1.0]
      epsilon: [0.1]
      kernel: ['rbf']
      gamma: ['scale']
      tol: [0.001]
      max_iter: [1000]

  - name: 'random_forest'
    enable: True
    params:
      n_estimators: [100]
      max_depth: [10]
      min_samples_split: [2]
      min_samples_leaf: [1]
      max_features: ['auto']
      bootstrap: [True]
      criterion: ['squared_error']
      max_leaf_nodes: [null]
      min_impurity_decrease: [0.0]
      warm_start: [False]
      n_jobs: [-1]
      random_state: [42]

  - name: 'gradient_boosting'
    enable: True
    params:
      n_estimators: [100]
      learning_rate: [0.1]
      max_depth: [3]
      min_samples_split: [2]
      min_samples_leaf: [1]
      subsample: [0.8]
      max_features: ['auto']
      criterion: ['friedman_mse']
      loss: ['squared_error']
      random_state: [42]

  - name: 'knn'
    enable: true
    params:
      n_neighbors: [5]
      weights: ['uniform']
      algorithm: ['auto']
      leaf_size: [30]
      p: [2]
      metric: ['minkowski']
      n_jobs: [-1]

# Hyperparameter Tuning
tuning:
  method: random
  cv_folds: 2
  n_iter: 2
  scoring: neg_mean_squared_error

# Model Evaluation
evaluation:
  metrics:
    - 'mse'
    - 'rmse'
    - 'mae'
    - 'r2'
    - 'explained_variance'
  analysis:
    learning_curves: True
    validation_curves: True
    feature_importance: True
    prediction_error: True
    residuals: True
    outliers: True
    cross_validation: True
  plots:
    residuals: True
    learning_curves: False  # Save time by disabling some plots
    validation_curves: False  # Save time by disabling some plots
    feature_importance: True
    prediction_error: True

# Output Configuration
output:
  save_model: True
  results_path: results
  model_path: models
  plots_path: plots
  log_level: INFO

# Hardware Configuration
hardware:
  use_gpu: False
  n_jobs: -1  # -1 for using all available cores